# components/chat_interface.py
import streamlit as st
import logging
import pandas as pd # For constructing prompt parts with DataFrame summaries
import os # Needed for os.path.basename
import re # For finding python-plot blocks
import matplotlib.pyplot as plt
import plotly.graph_objects as go
import plotly.express as px
import numpy as np # For exec context
from services.gemini_service import call_gemini_api
from services.prompt_builder import build_gemini_request_contents # Import the new prompt builder
from config.api_keys_config import api_keys_info # To get Gemini key names
# from config import app_settings # Might be needed if prompt_builder uses some settings directly

logger = logging.getLogger(__name__)

# Helper function to process and display model responses that might contain plots
def process_and_display_model_response_with_plots(response_text: str):
    """
    Processes the model's response text, displaying markdown for text parts
    and attempting to execute and render python-plot code blocks.
    """
    logger.debug(f"Processing model response for plot display. Response length: {len(response_text)}")
    # Pattern to find ```python-plot ... ``` blocks
    pattern = re.compile(r"```python-plot\n(.*?)\n```", re.DOTALL)
    parts = pattern.split(response_text)

    # Globals dict for exec, pre-populate with common libraries
    execution_globals = {
        'plt': plt,
        'go': go,
        'px': px,
        'pd': pd,
        'np': np,
        # 'st': st # Optionally, provide st if Gemini might generate st.write etc. For plots, usually not needed.
    }

    for i, part_content in enumerate(parts): # Renamed 'part' to 'part_content' to avoid conflict with pd.DataFrame.part
        if i % 2 == 0: # This is a text part
            if part_content.strip(): # Avoid rendering empty markdown parts
                st.markdown(part_content)
        else: # This is a plot code part
            plot_code = part_content.strip()
            if plot_code:
                logger.info(f"Executing python-plot code block:\n{plot_code[:300]}...") # Log snippet
                try:
                    # Using a fresh local scope for each execution
                    execution_locals = {}
                    exec(plot_code, execution_globals, execution_locals)

                    # Attempt to display Matplotlib plot
                    fig_mpl = execution_locals.get('fig', plt.gcf()) # Common practice to name the fig 'fig'

                    # Check if fig_mpl is a Matplotlib Figure and has axes (i.e., something was plotted)
                    if isinstance(fig_mpl, plt.Figure) and fig_mpl.get_axes():
                        logger.info("Displaying Matplotlib plot.")
                        st.pyplot(fig_mpl)
                    elif not isinstance(fig_mpl, plt.Figure): # If fig was something else, clear plt state
                        pass # Not a matplotlib figure from 'fig' variable or gcf() did not return one with axes

                    plt.clf() # Always clear the current figure state of plt
                    plt.close('all') # Close all figures plt might be tracking to free memory

                    # Attempt to display Plotly plot
                    # Check common names for Plotly figures in the executed code's locals
                    fig_plotly = None
                    if 'fig_plotly' in execution_locals:
                        fig_plotly = execution_locals['fig_plotly']
                    elif 'fig' in execution_locals and isinstance(execution_locals['fig'], go.Figure): # if 'fig' was used for plotly
                        fig_plotly = execution_locals['fig']

                    if fig_plotly and isinstance(fig_plotly, go.Figure):
                        logger.info("Displaying Plotly plot.")
                        st.plotly_chart(fig_plotly)
                    # else:
                        # logger.debug("No identifiable Plotly figure object found in execution locals.")

                except Exception as e_exec:
                    error_message = f"åŸ·è¡Œç¹ªåœ–ä»£ç¢¼æ™‚å‡ºéŒ¯ (Error executing plot code): {e_exec}"
                    logger.error(error_message, exc_info=True)
                    st.error(error_message)
                    st.code(plot_code, language="python") # Show the problematic code

def render_chat():
    """
    æ¸²æŸ“èŠå¤©ä»‹é¢ã€‚
    é¡¯ç¤ºèŠå¤©æ­·å²ã€è™•ç†ç”¨æˆ¶è¼¸å…¥ï¼Œä¸¦èª¿ç”¨ Gemini API ç”Ÿæˆå›æ‡‰ã€‚
    """
    logger.info("èŠå¤©ä»‹é¢ï¼šé–‹å§‹æ¸²æŸ“ (render_chat)ã€‚")

    st.header("ğŸ’¬ æ­¥é©Ÿä¸‰ï¼šèˆ‡ Gemini é€²è¡Œåˆ†æèˆ‡è¨è«–")

    # èŠå¤©è¨Šæ¯å®¹å™¨
    chat_container_height = st.session_state.get("chat_container_height", 400)
    chat_container = st.container(height=chat_container_height)
    logger.debug(f"èŠå¤©ä»‹é¢ï¼šèŠå¤©å®¹å™¨é«˜åº¦è¨­ç½®ç‚º {chat_container_height}pxã€‚")

    with chat_container:
        # session_state.chat_history æ‡‰ç”± session_state_manager åˆå§‹åŒ–
        if "chat_history" not in st.session_state or not isinstance(st.session_state.chat_history, list):
            logger.warning("èŠå¤©ä»‹é¢ï¼š'chat_history' æœªåœ¨ session_state ä¸­æ­£ç¢ºåˆå§‹åŒ–ï¼Œå°‡å…¶é‡ç½®ç‚ºç©ºåˆ—è¡¨ã€‚")
            st.session_state.chat_history = []

        for i, message in enumerate(st.session_state.chat_history):
            role = message.get("role", "model")
            avatar_icon = "ğŸ‘¤" if role == "user" else "âœ¨"
            with st.chat_message(role, avatar=avatar_icon):
                response_content = message["parts"][0]
                if message.get("is_error", False):
                    st.error(response_content)
                elif role == "model" and "```python-plot" in response_content:
                    # This message contains plot code, process it with the helper
                    process_and_display_model_response_with_plots(response_content)
                else:
                    # Standard markdown display for user messages or simple model responses
                    st.markdown(response_content)

    user_input = st.chat_input("å‘ Gemini æå•æˆ–çµ¦å‡ºæŒ‡ä»¤ï¼š", key="chat_user_input")

    if user_input:
        logger.info(f"èŠå¤©ä»‹é¢ï¼šæ”¶åˆ°ç”¨æˆ¶è¼¸å…¥ (é•·åº¦: {len(user_input)}): '{user_input[:70]}...'")
        st.session_state.chat_history.append({"role": "user", "parts": [user_input]})
        st.rerun()

    if st.session_state.chat_history and st.session_state.chat_history[-1]["role"] == "user" and not st.session_state.get("gemini_processing", False):
        st.session_state.gemini_processing = True
        logger.info("èŠå¤©ä»‹é¢ï¼šæª¢æ¸¬åˆ°æ–°çš„ç”¨æˆ¶è¨Šæ¯ï¼Œé–‹å§‹æº–å‚™ä¸¦èª¿ç”¨ Gemini APIã€‚")

        with st.spinner("Gemini æ­£åœ¨æ€è€ƒä¸­ï¼Œè«‹ç¨å€™..."):
            model_response_text = ""
            is_error_response = False

            try:
                logger.debug("èŠå¤©ä»‹é¢ï¼šé–‹å§‹æ§‹å»º Gemini API çš„æç¤ºè©åˆ—è¡¨ (prompt_parts)ã€‚")
                full_prompt_parts = []
                prompt_context_summary = [] # ç”¨æ–¼è¨˜éŒ„æç¤ºè©åŒ…å«å“ªäº›ä¸Šä¸‹æ–‡

                if st.session_state.get("main_gemini_prompt"):
                    full_prompt_parts.append(f"**ä¸»è¦æŒ‡ç¤º (System Prompt):**\n{st.session_state.main_gemini_prompt}")
                    prompt_context_summary.append("ç³»çµ±æç¤ºè©")

                # --- æ ¸å¿ƒåˆ†ææ–‡ä»¶è™•ç† (ä¾†è‡ª Wolf_Data æˆ–å·²ä¸Šå‚³æ–‡ä»¶) ---
                selected_core_doc_relative_paths = st.session_state.get("selected_core_documents", [])
                wolf_data_file_map = st.session_state.get("wolf_data_file_map", {})
                # uploaded_content_map remains for files uploaded via st.file_uploader, if we decide to allow mixing
                # For now, the logic in main_page.py prioritizes Wolf_Data for selection if available.
                # This chat_interface part will now prioritize resolving selected_core_doc_relative_paths using wolf_data_file_map.

                if selected_core_doc_relative_paths:
                    logger.info(f"èŠå¤©ä»‹é¢ï¼šæª¢æ¸¬åˆ° {len(selected_core_doc_relative_paths)} å€‹æ ¸å¿ƒæ–‡ä»¶è¢«é¸ä¸­ã€‚")
                    core_docs_content_parts = ["\n**æ ¸å¿ƒåˆ†ææ–‡ä»¶å…§å®¹ (Core Analysis Documents Content):**\n"]
                    files_actually_included = 0

                    for rel_path in selected_core_doc_relative_paths:
                        core_doc_full_path = wolf_data_file_map.get(rel_path)
                        doc_display_name = os.path.basename(rel_path) # Use relative path for display name initially

                        if core_doc_full_path and os.path.exists(core_doc_full_path):
                            doc_display_name = os.path.basename(core_doc_full_path) # Update to actual basename if path resolved
                            try:
                                if core_doc_full_path.endswith((".txt", ".md")):
                                    with open(core_doc_full_path, "r", encoding="utf-8") as f:
                                        content = f.read()
                                    core_docs_content_parts.append(f"--- Document Start: {doc_display_name} ---\n{content}\n--- Document End: {doc_display_name} ---\n\n")
                                    logger.debug(f"èŠå¤©ä»‹é¢ï¼šå·²å¾ '{core_doc_full_path}' æ·»åŠ æ ¸å¿ƒæ–‡ä»¶ '{doc_display_name}' (æ–‡æœ¬) åˆ°æç¤ºè©ã€‚é•·åº¦: {len(content)}")
                                    files_actually_included +=1
                                else:
                                    core_docs_content_parts.append(f"--- Document: {doc_display_name} (Unsupported file type for direct inclusion from Wolf_Data: {core_doc_full_path}) ---\n")
                                    logger.warning(f"èŠå¤©ä»‹é¢ï¼šæ ¸å¿ƒæ–‡ä»¶ '{doc_display_name}' ({core_doc_full_path}) é¡å‹ä¸å—æ”¯æŒï¼Œç„¡æ³•ç›´æ¥åŒ…å«ã€‚")
                            except FileNotFoundError:
                                core_docs_content_parts.append(f"--- Document: {doc_display_name} (File not found at path: {core_doc_full_path}) ---\n")
                                logger.error(f"èŠå¤©ä»‹é¢ï¼šæ ¸å¿ƒæ–‡ä»¶ '{doc_display_name}' è·¯å¾‘ '{core_doc_full_path}' æœªæ‰¾åˆ°ã€‚")
                            except Exception as e:
                                core_docs_content_parts.append(f"--- Document: {doc_display_name} (Error reading file at path: {core_doc_full_path}) ---\nError: {str(e)}\n")
                                logger.error(f"èŠå¤©ä»‹é¢ï¼šè®€å–æ ¸å¿ƒæ–‡ä»¶ '{doc_display_name}' ({core_doc_full_path}) æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}", exc_info=True)
                        elif rel_path in st.session_state.get("uploaded_file_contents", {}): # Fallback: Check if it's a name from uploaded_files_list
                            content = st.session_state.uploaded_file_contents[rel_path]
                            doc_display_name = rel_path # Use the key from uploaded_files_list
                            if isinstance(content, str):
                                core_docs_content_parts.append(f"--- Document Start (Uploaded): {doc_display_name} ---\n{content}\n--- Document End (Uploaded): {doc_display_name} ---\n\n")
                                logger.debug(f"èŠå¤©ä»‹é¢ï¼šå·²æ·»åŠ ä¾†è‡ª st.file_uploader çš„æ ¸å¿ƒæ–‡ä»¶ '{doc_display_name}' (æ–‡æœ¬) åˆ°æç¤ºè©ã€‚é•·åº¦: {len(content)}")
                            elif isinstance(content, pd.DataFrame):
                                core_docs_content_parts.append(f"--- Document Start (Uploaded): {doc_display_name} (è¡¨æ ¼æ•¸æ“š) ---\næ¬„ä½: {', '.join(content.columns)}\næ•¸æ“š (å‰5è¡Œ):\n{content.head(5).to_string()}\n--- Document End (Uploaded): {doc_display_name} ---\n\n")
                                logger.debug(f"èŠå¤©ä»‹é¢ï¼šå·²æ·»åŠ ä¾†è‡ª st.file_uploader çš„æ ¸å¿ƒæ–‡ä»¶ '{doc_display_name}' (è¡¨æ ¼) åˆ°æç¤ºè©ã€‚Shape: {content.shape}")
                            else:
                                core_docs_content_parts.append(f"--- Document (Uploaded): {doc_display_name} (äºŒé€²åˆ¶æˆ–å…¶ä»–æ ¼å¼ï¼Œç„¡æ³•ç›´æ¥é è¦½) ---\n[Content not directly previewable, length: {len(content)} bytes]\n--- Document End (Uploaded): {doc_display_name} ---\n\n")
                                logger.debug(f"èŠå¤©ä»‹é¢ï¼šå·²æ·»åŠ ä¾†è‡ª st.file_uploader çš„æ ¸å¿ƒæ–‡ä»¶ '{doc_display_name}' (äºŒé€²åˆ¶) åˆ°æç¤ºè©ã€‚é•·åº¦: {len(content)}")
                            files_actually_included += 1
                        else:
                            # This handles cases where rel_path is neither in wolf_data_file_map nor in uploaded_file_contents (e.g. mock name if main_page uses it)
                            core_docs_content_parts.append(f"--- Document: {rel_path} (Content not available from Wolf_Data or session uploads) ---\n")
                            logger.warning(f"èŠå¤©ä»‹é¢ï¼šæ ¸å¿ƒæ–‡ä»¶ '{rel_path}' åœ¨ selected_core_documents ä¸­ï¼Œä½†åœ¨ wolf_data_file_map æˆ– uploaded_file_contents ä¸­æœªæ‰¾åˆ°å…¶å…§å®¹ã€‚")

                    if files_actually_included > 0:
                        full_prompt_parts.append("".join(core_docs_content_parts))
                        prompt_context_summary.append(f"{files_actually_included}å€‹æ ¸å¿ƒåˆ†ææ–‡ä»¶")

                    logger.info("èŠå¤©ä»‹é¢ï¼šæ ¸å¿ƒåˆ†ææ–‡ä»¶è™•ç†å®Œç•¢ã€‚")
                    # Decision: If selected_core_documents is not empty, we ONLY use those.
                    # So, we skip the general uploaded_file_contents iteration.
                    logger.info("èŠå¤©ä»‹é¢ï¼šç”±æ–¼å·²é¸å®šæ ¸å¿ƒåˆ†ææ–‡ä»¶ï¼Œå°‡è·³éè‡ªå‹•åŒ…å«æ‰€æœ‰å…¶ä»– 'å·²ä¸Šå‚³æ–‡ä»¶åˆ—è¡¨(uploaded_file_contents)' çš„æ­¥é©Ÿã€‚")

                else: # No selected_core_doc_relative_paths, use general uploaded_file_contents if any
                    uploaded_content_map_general = st.session_state.get("uploaded_file_contents", {})
                    if uploaded_content_map_general:
                        logger.info("èŠå¤©ä»‹é¢ï¼šæœªé¸å®šæ ¸å¿ƒåˆ†ææ–‡ä»¶ï¼Œå°‡æª¢æŸ¥ä¸¦åŒ…å«æ‰€æœ‰ 'å·²ä¸Šå‚³æ–‡ä»¶åˆ—è¡¨(uploaded_file_contents)' ä¸­çš„æ–‡ä»¶ã€‚")
                        num_general_uploaded_files = len(uploaded_content_map_general)
                        uploaded_texts_str = "**å·²ä¸Šå‚³æ–‡ä»¶å…§å®¹æ‘˜è¦ (All Uploaded Files Summary):**\n"
                        for fn, content in uploaded_content_map_general.items():
                            if isinstance(content, str):
                                uploaded_texts_str += f"æª”å: {fn}\nå…§å®¹ç‰‡æ®µ (å‰1000å­—ç¬¦):\n{content[:1000]}...\n\n"
                            elif isinstance(content, pd.DataFrame):
                                 uploaded_texts_str += f"æª”å: {fn} (è¡¨æ ¼æ•¸æ“š)\næ¬„ä½: {', '.join(content.columns)}\nå‰3è¡Œ:\n{content.head(3).to_string()}\n\n"
                            else: # å…¶ä»–é¡å‹ï¼Œå¦‚ bytes
                                uploaded_texts_str += f"æª”å: {fn} (äºŒé€²åˆ¶æˆ–å…¶ä»–æ ¼å¼ï¼Œç„¡æ³•ç›´æ¥é è¦½)\n\n"
                        full_prompt_parts.append(uploaded_texts_str)
                        prompt_context_summary.append(f"{num_general_uploaded_files}å€‹å·²ä¸Šå‚³æª”æ¡ˆ (é€šç”¨)")

                # --- å¤–éƒ¨æ•¸æ“šè™•ç† (ä¸è®Š) ---
                if st.session_state.get("fetched_data_preview") and isinstance(st.session_state.fetched_data_preview, dict):
                    num_data_sources = len(st.session_state.fetched_data_preview)
                    if num_data_sources > 0:
                        external_data_str = "**å·²å¼•å…¥çš„å¤–éƒ¨å¸‚å ´èˆ‡ç¸½ç¶“æ•¸æ“šæ‘˜è¦:**\n"
                        for source, data_items in st.session_state.fetched_data_preview.items():
                            external_data_str += f"\nä¾†æº: {source.upper()}\n"
                            if isinstance(data_items, dict):
                                for item_name, df_item in data_items.items():
                                    if isinstance(df_item, pd.DataFrame):
                                        external_data_str += f"  {item_name} (æœ€è¿‘3ç­†):\n{df_item.tail(3).to_string(index=False)}\n" # index=False æ›´ç°¡æ½”
                            elif isinstance(data_items, pd.DataFrame):
                                external_data_str += f"  {source}æ•¸æ“š (æœ€è¿‘3ç­†):\n{data_items.tail(3).to_string(index=False)}\n"
                        full_prompt_parts.append(external_data_str)
                        prompt_context_summary.append(f"{num_data_sources}å€‹å¤–éƒ¨æ•¸æ“šæº")

                last_user_input = st.session_state.chat_history[-1]["parts"][0]
                full_prompt_parts.append(f"**ä½¿ç”¨è€…ç•¶å‰å•é¡Œ/æŒ‡ä»¤:**\n{last_user_input}")
                prompt_context_summary.append("ç”¨æˆ¶ç•¶å‰å•é¡Œ")

                final_prompt_for_api = "\n---\n".join(map(str, full_prompt_parts))
                final_prompt_length = len(final_prompt_for_api)
                logger.info(f"èŠå¤©ä»‹é¢ï¼šGemini æç¤ºè©ä¸Šä¸‹æ–‡åŒ…å«: {', '.join(prompt_context_summary)}ã€‚æœ€çµ‚æç¤ºè©é•·åº¦: {final_prompt_length} å­—å…ƒã€‚")
                if final_prompt_length > 100000: # Example threshold for very long prompt
                    logger.warning(f"èŠå¤©ä»‹é¢ï¼šæœ€çµ‚æç¤ºè©é•·åº¦ ({final_prompt_length} å­—å…ƒ) éå¸¸é•·ï¼Œå¯èƒ½å½±éŸ¿ API æ€§èƒ½æˆ–æˆæœ¬ã€‚")

                valid_gemini_api_keys = [
                    st.session_state.get(key_name, "")
                    for key_name in api_keys_info.values()
                    if "gemini" in key_name.lower() and st.session_state.get(key_name, "") # ç¢ºä¿æ¯”è¼ƒæ™‚å¤§å°å¯«ä¸æ•æ„Ÿ
                ]

                selected_model_name = st.session_state.get("selected_model_name")
                # ç¢ºä¿ generation_config æ˜¯å¾ session_state ç²å–ï¼Œå¦‚æœä¸å­˜åœ¨å‰‡ä½¿ç”¨é è¨­å€¼
                generation_config = st.session_state.get("generation_config", {"temperature": 0.7, "top_p": 0.9, "top_k": 32, "max_output_tokens": 8192})
                selected_cache_name_for_api = st.session_state.get("selected_cache_for_generation")

                if not valid_gemini_api_keys:
                    model_response_text = "éŒ¯èª¤ï¼šè«‹åœ¨å´é‚Šæ¬„è¨­å®šè‡³å°‘ä¸€å€‹æœ‰æ•ˆçš„ Gemini API é‡‘é‘°ã€‚"
                    is_error_response = True
                    logger.error("èŠå¤©ä»‹é¢ï¼šç„¡æœ‰æ•ˆ Gemini API é‡‘é‘°ï¼Œç„¡æ³•èª¿ç”¨ APIã€‚")
                elif not selected_model_name:
                    model_response_text = "éŒ¯èª¤ï¼šè«‹åœ¨å´é‚Šæ¬„é¸æ“‡ä¸€å€‹ Gemini æ¨¡å‹ã€‚"
                    is_error_response = True
                    logger.error("èŠå¤©ä»‹é¢ï¼šæœªé¸æ“‡ Gemini æ¨¡å‹ï¼Œç„¡æ³•èª¿ç”¨ APIã€‚")
                else:
                    logger.info(f"èŠå¤©ä»‹é¢ï¼šæº–å‚™èª¿ç”¨ call_gemini_apiã€‚æ¨¡å‹: '{selected_model_name}', æœ‰æ•ˆé‡‘é‘°æ•¸é‡: {len(valid_gemini_api_keys)}, ä½¿ç”¨å¿«å–: '{selected_cache_name_for_api if selected_cache_name_for_api else 'å¦'}'")
                    logger.debug(f"èŠå¤©ä»‹é¢ï¼šå‚³éçµ¦ call_gemini_api çš„ generation_config: {generation_config}")

                    # Note: call_gemini_api expects a list of parts, not a single pre-joined string.
                    # However, our current structure of full_prompt_parts already builds it as a list of strings.
                    # If call_gemini_api was more nuanced about parts, this might need adjustment.
                    # For now, it implies call_gemini_api internally joins them or handles a list of strings.
                    # Let's assume call_gemini_api joins the list elements with newlines if necessary.
                    # The prompt_parts argument in call_gemini_api should indeed be a list of strings.
                    model_response_text = call_gemini_api(
                        prompt_parts=full_prompt_parts, # Passing the list of strings
                        api_keys_list=valid_gemini_api_keys,
                        selected_model=selected_model_name,
                        global_rpm=st.session_state.get("global_rpm_limit", 3),
                        global_tpm=st.session_state.get("global_tpm_limit", 100000),
                        generation_config_dict=generation_config,
                        cached_content_name=selected_cache_name_for_api
                    )

                    if model_response_text.startswith("éŒ¯èª¤ï¼š"):
                        is_error_response = True
                        logger.warning(f"èŠå¤©ä»‹é¢ï¼šGemini API èª¿ç”¨è¿”å›æ¥­å‹™éŒ¯èª¤è¨Šæ¯: {model_response_text[:200]}...") #è¨˜éŒ„éƒ¨åˆ†éŒ¯èª¤ä¿¡æ¯
                    else:
                        logger.info(f"èŠå¤©ä»‹é¢ï¼šGemini API æˆåŠŸè¿”å›éŸ¿æ‡‰ã€‚éŸ¿æ‡‰é•·åº¦: {len(model_response_text)}ã€‚")
                        logger.debug(f"èŠå¤©ä»‹é¢ï¼šGemini API éŸ¿æ‡‰ (å‰100å­—ç¬¦): {model_response_text[:100]}...")


            except Exception as e:
                logger.error(f"èŠå¤©ä»‹é¢ï¼šåœ¨æº–å‚™æˆ–èª¿ç”¨ Gemini API æ™‚ç™¼ç”Ÿæ„å¤–éŒ¯èª¤: {type(e).__name__} - {str(e)}", exc_info=True)
                model_response_text = f"è™•ç†æ‚¨çš„è«‹æ±‚æ™‚ç™¼ç”Ÿæœªé æœŸçš„éŒ¯èª¤ï¼š {type(e).__name__} - {str(e)}"
                is_error_response = True

            logger.debug(f"èŠå¤©ä»‹é¢ï¼šå°‡æ¨¡å‹å›æ‡‰æ·»åŠ åˆ°èŠå¤©æ­·å²ã€‚æ˜¯å¦ç‚ºéŒ¯èª¤: {is_error_response}ã€‚")
            st.session_state.chat_history.append({
                "role": "model",
                "parts": [model_response_text],
                "is_error": is_error_response
            })
            st.session_state.gemini_processing = False
            logger.info("èŠå¤©ä»‹é¢ï¼šGemini API è™•ç†å®Œç•¢ï¼Œæº–å‚™åˆ·æ–°é é¢é¡¯ç¤ºçµæœã€‚")
            st.rerun()

    logger.info("èŠå¤©ä»‹é¢ï¼šæ¸²æŸ“çµæŸ (render_chat)ã€‚")

def _trigger_gemini_response_processing():
    """
    Internal function to process the last user message in chat_history and get Gemini's response.
    This function assumes st.session_state.gemini_processing is already True.
    """
    logger.info("èŠå¤©ä»‹é¢ (_trigger_gemini_response_processing)ï¼šé–‹å§‹æº–å‚™ä¸¦èª¿ç”¨ Gemini APIã€‚")
    with st.spinner("Gemini æ­£åœ¨æ€è€ƒä¸­ï¼Œè«‹ç¨å€™..."):
        model_response_text = ""
        is_error_response = False
        try:
            logger.debug("èŠå¤©ä»‹é¢ï¼šé–‹å§‹æ§‹å»º Gemini API çš„æç¤ºè©åˆ—è¡¨ (prompt_parts)ã€‚")
            full_prompt_parts = []
            prompt_context_summary = []

            if st.session_state.get("main_gemini_prompt"):
                full_prompt_parts.append(f"**ä¸»è¦æŒ‡ç¤º (System Prompt):**\n{st.session_state.main_gemini_prompt}")
                prompt_context_summary.append("ç³»çµ±æç¤ºè©")

            selected_core_doc_relative_paths = st.session_state.get("selected_core_documents", [])
            wolf_data_file_map = st.session_state.get("wolf_data_file_map", {})

            if selected_core_doc_relative_paths:
                logger.info(f"èŠå¤©ä»‹é¢ï¼šæª¢æ¸¬åˆ° {len(selected_core_doc_relative_paths)} å€‹æ ¸å¿ƒæ–‡ä»¶è¢«é¸ä¸­ã€‚")
                core_docs_content_parts = ["\n**æ ¸å¿ƒåˆ†ææ–‡ä»¶å…§å®¹ (Core Analysis Documents Content):**\n"]
                files_actually_included = 0
                for rel_path in selected_core_doc_relative_paths:
                    core_doc_full_path = wolf_data_file_map.get(rel_path)
                    doc_display_name = os.path.basename(rel_path)
                    if core_doc_full_path and os.path.exists(core_doc_full_path):
                        doc_display_name = os.path.basename(core_doc_full_path)
                        try:
                            if core_doc_full_path.endswith((".txt", ".md")):
                                with open(core_doc_full_path, "r", encoding="utf-8") as f: content = f.read()
                                core_docs_content_parts.append(f"--- Document Start: {doc_display_name} ---\n{content}\n--- Document End: {doc_display_name} ---\n\n")
                                files_actually_included +=1
                            else:
                                core_docs_content_parts.append(f"--- Document: {doc_display_name} (Unsupported file type from Wolf_Data: {core_doc_full_path}) ---\n")
                        except Exception as e:
                            core_docs_content_parts.append(f"--- Document: {doc_display_name} (Error reading file: {core_doc_full_path}) ---\nError: {str(e)}\n")
                    elif rel_path in st.session_state.get("uploaded_file_contents", {}):
                        content = st.session_state.uploaded_file_contents[rel_path]
                        doc_display_name = rel_path
                        if isinstance(content, str):
                            core_docs_content_parts.append(f"--- Document Start (Uploaded): {doc_display_name} ---\n{content}\n--- Document End (Uploaded): {doc_display_name} ---\n\n")
                        elif isinstance(content, pd.DataFrame):
                            core_docs_content_parts.append(f"--- Document Start (Uploaded): {doc_display_name} (è¡¨æ ¼æ•¸æ“š) ---\næ¬„ä½: {', '.join(content.columns)}\næ•¸æ“š (å‰5è¡Œ):\n{content.head(5).to_string()}\n--- Document End (Uploaded): {doc_display_name} ---\n\n")
                        else:
                            core_docs_content_parts.append(f"--- Document (Uploaded): {doc_display_name} (äºŒé€²åˆ¶æˆ–å…¶ä»–æ ¼å¼) ---\n[Content not directly previewable, length: {len(content)} bytes]\n--- Document End (Uploaded): {doc_display_name} ---\n\n")
                        files_actually_included += 1
                    else:
                        core_docs_content_parts.append(f"--- Document: {rel_path} (Content not available) ---\n")
                if files_actually_included > 0:
                    full_prompt_parts.append("".join(core_docs_content_parts))
                    prompt_context_summary.append(f"{files_actually_included}å€‹æ ¸å¿ƒåˆ†ææ–‡ä»¶")
                logger.info("èŠå¤©ä»‹é¢ï¼šæ ¸å¿ƒåˆ†ææ–‡ä»¶è™•ç†å®Œç•¢ã€‚")
            else:
                uploaded_content_map_general = st.session_state.get("uploaded_file_contents", {})
                if uploaded_content_map_general:
                    num_general_uploaded_files = len(uploaded_content_map_general)
                    uploaded_texts_str = "**å·²ä¸Šå‚³æ–‡ä»¶å…§å®¹æ‘˜è¦ (All Uploaded Files Summary):**\n"
                    for fn, content in uploaded_content_map_general.items():
                        if isinstance(content, str): uploaded_texts_str += f"æª”å: {fn}\nå…§å®¹ç‰‡æ®µ (å‰1000å­—ç¬¦):\n{content[:1000]}...\n\n"
                        elif isinstance(content, pd.DataFrame): uploaded_texts_str += f"æª”å: {fn} (è¡¨æ ¼æ•¸æ“š)\næ¬„ä½: {', '.join(content.columns)}\nå‰3è¡Œ:\n{content.head(3).to_string()}\n\n"
                        else: uploaded_texts_str += f"æª”å: {fn} (äºŒé€²åˆ¶æˆ–å…¶ä»–æ ¼å¼)\n\n"
                    full_prompt_parts.append(uploaded_texts_str)
                    prompt_context_summary.append(f"{num_general_uploaded_files}å€‹å·²ä¸Šå‚³æª”æ¡ˆ (é€šç”¨)")

            if st.session_state.get("fetched_data_preview") and isinstance(st.session_state.fetched_data_preview, dict):
                num_data_sources = len(st.session_state.fetched_data_preview)
                if num_data_sources > 0:
                    external_data_str = "**å·²å¼•å…¥çš„å¤–éƒ¨å¸‚å ´èˆ‡ç¸½ç¶“æ•¸æ“šæ‘˜è¦:**\n"
                    for source, data_items in st.session_state.fetched_data_preview.items():
                        external_data_str += f"\nä¾†æº: {source.upper()}\n"
                        if isinstance(data_items, dict):
                            for item_name, df_item in data_items.items():
                                if isinstance(df_item, pd.DataFrame): external_data_str += f"  {item_name} (æœ€è¿‘3ç­†):\n{df_item.tail(3).to_string(index=False)}\n"
                        elif isinstance(data_items, pd.DataFrame): external_data_str += f"  {source}æ•¸æ“š (æœ€è¿‘3ç­†):\n{data_items.tail(3).to_string(index=False)}\n"
                    full_prompt_parts.append(external_data_str)
                    prompt_context_summary.append(f"{num_data_sources}å€‹å¤–éƒ¨æ•¸æ“šæº")

            last_user_input = st.session_state.chat_history[-1]["parts"][0]
            full_prompt_parts.append(f"**ä½¿ç”¨è€…ç•¶å‰å•é¡Œ/æŒ‡ä»¤:**\n{last_user_input}")
            prompt_context_summary.append("ç”¨æˆ¶ç•¶å‰å•é¡Œ")

            # final_prompt_for_api = "\n---\n".join(map(str, full_prompt_parts)) # This was for single string prompt
            # The call_gemini_api expects a list of parts.
            # Ensure full_prompt_parts is a list of strings/parts suitable for the API.

            final_prompt_length = sum(len(str(p)) for p in full_prompt_parts)
            logger.info(f"èŠå¤©ä»‹é¢ï¼šGemini æç¤ºè©ä¸Šä¸‹æ–‡åŒ…å«: {', '.join(prompt_context_summary)}ã€‚æœ€çµ‚æç¤ºè©é•·åº¦: {final_prompt_length} å­—å…ƒã€‚")

            valid_gemini_api_keys = [st.session_state.get(key_name, "") for key_name in api_keys_info.values() if "gemini" in key_name.lower() and st.session_state.get(key_name, "")]
            selected_model_name = st.session_state.get("selected_model_name")
            generation_config = st.session_state.get("generation_config", {"temperature": 0.7, "top_p": 0.9, "top_k": 32, "max_output_tokens": 8192})
            selected_cache_name_for_api = st.session_state.get("selected_cache_for_generation")

            if not valid_gemini_api_keys:
                model_response_text = "éŒ¯èª¤ï¼šè«‹åœ¨å´é‚Šæ¬„è¨­å®šè‡³å°‘ä¸€å€‹æœ‰æ•ˆçš„ Gemini API é‡‘é‘°ã€‚"
                is_error_response = True
            elif not selected_model_name:
                model_response_text = "éŒ¯èª¤ï¼šè«‹åœ¨å´é‚Šæ¬„é¸æ“‡ä¸€å€‹ Gemini æ¨¡å‹ã€‚"
                is_error_response = True
            else:
                model_response_text = call_gemini_api(
                    prompt_parts=full_prompt_parts, # This should be the list of strings
                    api_keys_list=valid_gemini_api_keys,
                    selected_model=selected_model_name,
                    global_rpm=st.session_state.get("global_rpm_limit", 3),
                    global_tpm=st.session_state.get("global_tpm_limit", 100000),
                    generation_config_dict=generation_config,
                    cached_content_name=selected_cache_name_for_api
                )
                if model_response_text.startswith("éŒ¯èª¤ï¼š"): is_error_response = True
        except Exception as e:
            logger.error(f"èŠå¤©ä»‹é¢ï¼šåœ¨æº–å‚™æˆ–èª¿ç”¨ Gemini API æ™‚ç™¼ç”Ÿæ„å¤–éŒ¯èª¤: {type(e).__name__} - {str(e)}", exc_info=True)
            model_response_text = f"è™•ç†æ‚¨çš„è«‹æ±‚æ™‚ç™¼ç”Ÿæœªé æœŸçš„éŒ¯èª¤ï¼š {type(e).__name__} - {str(e)}"
            is_error_response = True

        st.session_state.chat_history.append({"role": "model", "parts": [model_response_text], "is_error": is_error_response})
        st.session_state.gemini_processing = False
        logger.info("èŠå¤©ä»‹é¢ï¼šGemini API è™•ç†å®Œç•¢ï¼Œæº–å‚™åˆ·æ–°é é¢é¡¯ç¤ºçµæœã€‚")
        st.rerun()

def add_message_and_process(user_prompt_content: str, from_agent: bool = False):
    """
    Adds a user message to the chat history and triggers the Gemini processing flow via st.rerun().
    If 'from_agent' is True, it implies st.session_state.current_agent_prompt was set and might be cleared here or after processing.
    """
    logger.info(f"èŠå¤©ä»‹é¢ (add_message_and_process)ï¼šæ”¶åˆ°æ–°çš„ç”¨æˆ¶æç¤º (ä¾†è‡ª Agent: {from_agent})ã€‚é•·åº¦: {len(user_prompt_content)}")
    if 'chat_history' not in st.session_state:
        st.session_state.chat_history = []
    st.session_state.chat_history.append({"role": "user", "parts": [user_prompt_content]})

    if from_agent:
        # Clear the current_agent_prompt as it's now been transferred to chat_history
        st.session_state.current_agent_prompt = ""
        logger.debug("èŠå¤©ä»‹é¢ (add_message_and_process)ï¼šå·²æ¸…é™¤ current_agent_prompt (å›  from_agent=True)ã€‚")

    # The actual processing will be picked up by render_chat after st.rerun()
    st.rerun()


def render_chat():
    """
    æ¸²æŸ“èŠå¤©ä»‹é¢ã€‚
    é¡¯ç¤ºèŠå¤©æ­·å²ã€è™•ç†ç”¨æˆ¶è¼¸å…¥ï¼Œä¸¦èª¿ç”¨ Gemini API ç”Ÿæˆå›æ‡‰ã€‚
    """
    logger.info("èŠå¤©ä»‹é¢ï¼šé–‹å§‹æ¸²æŸ“ (render_chat)ã€‚")

    st.header("ğŸ’¬ æ­¥é©Ÿä¸‰ï¼šèˆ‡ Gemini é€²è¡Œåˆ†æèˆ‡è¨è«–")

    chat_container_height = st.session_state.get("chat_container_height", 400)
    chat_container = st.container(height=chat_container_height)
    logger.debug(f"èŠå¤©ä»‹é¢ï¼šèŠå¤©å®¹å™¨é«˜åº¦è¨­ç½®ç‚º {chat_container_height}pxã€‚")

    with chat_container:
        if "chat_history" not in st.session_state or not isinstance(st.session_state.chat_history, list):
            logger.warning("èŠå¤©ä»‹é¢ï¼š'chat_history' æœªåœ¨ session_state ä¸­æ­£ç¢ºåˆå§‹åŒ–ï¼Œå°‡å…¶é‡ç½®ç‚ºç©ºåˆ—è¡¨ã€‚")
            st.session_state.chat_history = []

        for i, message in enumerate(st.session_state.chat_history):
            role = message.get("role", "model")
            avatar_icon = "ğŸ‘¤" if role == "user" else "âœ¨"
            with st.chat_message(role, avatar=avatar_icon):
                response_content = message["parts"][0]
                if message.get("is_error", False):
                    st.error(response_content)
                elif role == "model" and "```python-plot" in response_content:
                    process_and_display_model_response_with_plots(response_content)
                else:
                    st.markdown(response_content)

    user_input = st.chat_input("å‘ Gemini æå•æˆ–çµ¦å‡ºæŒ‡ä»¤ï¼š", key="chat_user_input")

    if user_input:
        # Instead of directly appending and rerunning, call the new helper
        add_message_and_process(user_input, from_agent=False)

    # This block will now call the refactored processing function
    if st.session_state.chat_history and \
       st.session_state.chat_history[-1]["role"] == "user" and \
       not st.session_state.get("gemini_processing", False):
        st.session_state.gemini_processing = True # Mark as processing
        _trigger_gemini_response_processing() # Call the refactored internal function

    # logger.info("èŠå¤©ä»‹é¢ï¼šæ¸²æŸ“çµæŸ (render_chat)ã€‚") # Moved to the start of the function for this turn


if __name__ == "__main__":
    # ç‚ºäº†èƒ½åœ¨æœ¬åœ°é‹è¡Œå’Œæ¸¬è©¦æ­¤çµ„ä»¶ï¼Œéœ€è¦æ¨¡æ“¬ Streamlit å’Œ session_state
    # ä»¥åŠç›¸é—œçš„æœå‹™å’Œé…ç½®ã€‚
    # é€™é€šå¸¸æ¯”è¼ƒè¤‡é›œï¼Œæ›´é©åˆåœ¨é›†æˆåˆ°ä¸»æ‡‰ç”¨å¾Œé€²è¡Œç«¯åˆ°ç«¯æ¸¬è©¦ã€‚
    # ä»¥ä¸‹æ˜¯ä¸€å€‹éå¸¸åŸºç¤çš„æ¨¡æ“¬ç¤ºä¾‹ï¼š

    # class MockSessionState:
    #     def __init__(self, initial_state=None):
    #         self._state = initial_state if initial_state else {}
    #     def get(self, key, default=None): return self._state.get(key, default)
    #     def __setitem__(self, key, value): self._state[key] = value
    #     def __getitem__(self, key): return self._state[key]
    #     def __contains__(self, key): return key in self._state
    #     def update(self, d): self._state.update(d)

    # st.session_state = MockSessionState({
    #     "chat_history": [],
    #     "main_gemini_prompt": "Test prompt",
    #     "uploaded_file_contents": {"test.txt": "File content"},
    #     "fetched_data_preview": {"yfinance": {"AAPL": pd.DataFrame({'Close': [150, 151]})}},
    #     "gemini_api_key_1": "FAKE_KEY", # éœ€è¦çœŸå¯¦æˆ–æ¨¡æ“¬çš„ API key æ‰èƒ½å®Œæ•´æ¸¬è©¦
    #     "selected_model_name": "gemini-1.0-pro",
    #     "global_rpm_limit": 3,
    #     "global_tpm_limit": 100000,
    #     "active_gemini_key_index": 0,
    #     "gemini_api_key_usage": {},
    #     "api_keys_info": {"Gemini API Key 1": "gemini_api_key_1"}
    # })

    # æ¨¡æ“¬ Streamlit UI å…ƒç´ 
    # st.header = print
    # st.container = lambda height: MockContainer() # ç°¡æ˜“æ¨¡æ“¬
    # st.chat_message = lambda role, avatar: MockChatMessage(role, avatar) # ç°¡æ˜“æ¨¡æ“¬
    # st.markdown = print
    # st.error = print
    # st.chat_input = lambda label, key: input(label) # ç°¡æ˜“æ¨¡æ“¬
    # st.spinner = lambda text: MockSpinner(text) # ç°¡æ˜“æ¨¡æ“¬
    # st.rerun = lambda: print("--- ST.RERUN CALLED ---")

    # class MockContainer:
    #     def __enter__(self): return self
    #     def __exit__(self, type, value, traceback): pass

    # class MockChatMessage:
    #     def __init__(self, role, avatar): self.role = role; self.avatar = avatar
    #     def __enter__(self): print(f"Entering chat message: {self.role}, Avatar: {self.avatar}"); return self
    #     def __exit__(self, type, value, traceback): print(f"Exiting chat message: {self.role}")

    # class MockSpinner:
    #     def __init__(self, text): self.text = text
    #     def __enter__(self): print(f"Spinner started: {self.text}"); return self
    #     def __exit__(self, type, value, traceback): print("Spinner finished.")

    # render_chat()
    pass
