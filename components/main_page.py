# components/main_page.py
import streamlit as st
import logging
import pandas as pd # For displaying DataFrame previews
import os # Ensure os is imported for path operations
# Import the new helper function
from components.chat_interface import add_message_and_process
# Import for Colab Drive path checking
from utils.colab_utils import ensure_colab_drive_mount_if_needed # Changed import path
import os
import sys
import requests # Added for API calls
# from utils.path_manager import SOURCE_DOCS_DIR, IN_COLAB # Already imported via file_processors or similar, ensure it's available
from utils.path_manager import SOURCE_DOCS_DIR, IN_COLAB # Explicitly ensure it's here if direct use
# Removed old data_fetchers
# from services.data_fetchers import (
#     fetch_yfinance_data,
#     fetch_fred_data,
#     fetch_ny_fed_data
# )
# from config.api_keys_config import api_keys_info # For checking Gemini keys - This check might change or be removed
from config import app_settings # Import app_settings primarily for FASTAPI_BACKEND_URL if not sourced otherwise
# Specific constant imports below will be removed or changed to use st.session_state.ui_settings
# from config.app_settings import ALLOWED_UPLOAD_FILE_TYPES, TEXT_PREVIEW_MAX_CHARS, CORE_DOC_SCAN_EXTENSIONS, AGENT_BUTTONS_PER_ROW

logger = logging.getLogger(__name__)

# def _are_gemini_keys_set() -> bool: # This function might be deprecated if key handling is fully backend
#     """æª¢æŸ¥ session_state ä¸­æ˜¯å¦è¨­ç½®äº†ä»»ä½• Gemini API é‡‘é‘°ã€‚"""
#     # This check is less relevant now as keys are backend managed.
#     # For now, we can assume if backend is up, it's configured.
#     # Or, this could be replaced by a backend health check / config check endpoint.
#     # For this refactor, we'll remove its direct usage in restricting UI elements,
#     # as functionality now depends on backend calls.
#     # from config.api_keys_config import api_keys_info
#     # for key_label, key_name in api_keys_info.items():
#     #     if "gemini" in key_label.lower() and st.session_state.get(key_name): # This assumes keys were in session_state
#     #         return True
#     return True # Assume backend is configured for now, or this check is removed.

def render_main_page_content():
    """
    æ¸²æŸ“ä¸»é é¢çš„UIå…§å®¹ã€‚
    åŒ…æ‹¬æ¨™é¡Œã€æ–‡ä»¶ä¸Šå‚³ã€å¤–éƒ¨æ•¸æ“šæºé¸æ“‡å’Œç²å–ã€æ•¸æ“šé è¦½å’ŒéŒ¯èª¤é¡¯ç¤ºç­‰ã€‚
    """
    logger.info("ä¸»é é¢ï¼šé–‹å§‹æ¸²æŸ“ä¸»è¦å…§å®¹ (render_main_page_content)ã€‚")

    # --- Handle Colab Drive Mount Prompt ---
    # This should be near the top to potentially block other rendering or inform user early.
    if st.session_state.get("show_drive_mount_prompt", False):
        path_requested = st.session_state.get("drive_path_requested", "æŒ‡å®šçš„è·¯å¾‘")
        st.warning(
            f"âš ï¸ **Google Drive è·¯å¾‘æœªæ›è¼‰æˆ–ä¸å¯ç”¨**\n\n"
            f"æ‡‰ç”¨ç¨‹å¼éœ€è¦å­˜å– Google Drive ä¸Šçš„è·¯å¾‘ '{path_requested}'ï¼Œä½†è©²è·¯å¾‘ç›®å‰ç„¡æ³•è¨ªå•ã€‚\n\n"
            "å¦‚æœæ‚¨æ­£åœ¨ Colab ç’°å¢ƒä¸­é‹è¡Œæ­¤æ‡‰ç”¨ç¨‹å¼ï¼Œè«‹åŸ·è¡Œç­†è¨˜æœ¬ä¸­çš„ Google Drive æ›è¼‰å„²å­˜æ ¼ï¼Œç„¶å¾Œé»æ“Šä¸‹æ–¹çš„ã€Œé‡è©¦æª”æ¡ˆå­˜å–ã€æŒ‰éˆ•ã€‚",
            icon="ğŸ“"
        )
        if st.button("ğŸ”„ é‡è©¦æª”æ¡ˆå­˜å–", key="drive_retry_button"):
            st.session_state.show_drive_mount_prompt = False # Clear the flag
            # We don't need to set drive_path_requested to "" here,
            # ensure_colab_drive_mount_if_needed will re-check and re-set if still not mounted.
            logger.info("ç”¨æˆ¶é»æ“Š 'é‡è©¦æª”æ¡ˆå­˜å–' æŒ‰éˆ•ã€‚é‡æ–°é‹è¡Œé é¢ã€‚")
            st.rerun()
        # Optionally, you might want to stop further rendering of components that depend on this path
        # For now, the check before os.walk will handle not trying to access it.

    # --- Initialize session state for this page if not already done ---
    if "selected_core_documents" not in st.session_state:
        st.session_state.selected_core_documents = []
        logger.debug("ä¸»é é¢ï¼š'selected_core_documents' åˆå§‹åŒ–ç‚ºç©ºåˆ—è¡¨ã€‚")
    if "wolf_data_file_map" not in st.session_state: # For storing relative_path -> full_path
        st.session_state.wolf_data_file_map = {}
        logger.debug("ä¸»é é¢ï¼š'wolf_data_file_map' åˆå§‹åŒ–ç‚ºç©ºå­—å…¸ã€‚")

    st.title("é‡‘èåˆ†æèˆ‡æ´å¯ŸåŠ©ç† (ç”± Gemini é©…å‹•)")

    # --- Agent å¿«æ·ä»»å‹™ ---
    st.subheader("ğŸ¤– Agent å¿«æ·ä»»å‹™")
    ui_settings = st.session_state.get("ui_settings", {})
    prompts_dir = ui_settings.get("prompts_dir", "prompts") # Fallback to "prompts"
    agent_buttons_per_row = ui_settings.get("agent_buttons_per_row", 3)

    agent_prompt_files = []
    if os.path.exists(prompts_dir) and os.path.isdir(prompts_dir):
        try:
            agent_prompt_files = sorted([
                f for f in os.listdir(prompts_dir)
                if f.startswith("agent_") and f.endswith(".txt")
            ]) # Sort for consistent order
        except Exception as e:
            logger.error(f"ä¸»é é¢ï¼šè®€å– prompts ç›®éŒ„ '{prompts_dir}' æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            st.error(f"ç„¡æ³•è®€å– Agent ä»»å‹™åˆ—è¡¨: {e}")

    if not agent_prompt_files:
        st.info(f"ç›®å‰æ²’æœ‰å¯ç”¨çš„ Agent å¿«æ·ä»»å‹™ã€‚è«‹åœ¨ '{prompts_dir}' æ–‡ä»¶å¤¾ä¸‹æ·»åŠ  'agent_*.txt' æ–‡ä»¶ã€‚")
    else:
        # Determine number of columns, e.g., 3 or 4 per row
        num_cols = min(len(agent_prompt_files), agent_buttons_per_row) # Max buttons per row from settings
        cols = st.columns(num_cols)
        for i, prompt_file in enumerate(agent_prompt_files):
            button_label = prompt_file.replace("agent_", "").replace(".txt", "").replace("_", " ").title()
            # Use session state to manage button clicks to avoid issues with st.rerun if called from elsewhere
            button_key = f"agent_button_{prompt_file}"

            if cols[i % num_cols].button(button_label, key=button_key):
                logger.info(f"ä¸»é é¢ï¼šAgent æŒ‰éˆ• '{button_label}' è¢«é»æ“Šã€‚")
                try:
                    with open(os.path.join(prompts_dir, prompt_file), "r", encoding="utf-8") as f:
                        prompt_content = f.read()

                    # Store the raw prompt. It will be cleared by add_message_and_process if from_agent is True.
                    st.session_state.current_agent_prompt = prompt_content

                    logger.info(f"Agent ä»»å‹™ '{button_label}' çš„æç¤ºè©å·²è¼‰å…¥ã€‚èª¿ç”¨ add_message_and_processã€‚")

                    # Call the helper function from chat_interface.py
                    add_message_and_process(prompt_content, from_agent=True)

                    # Toast is good, st.rerun() is handled by add_message_and_process
                    st.toast(f"Agent ä»»å‹™ '{button_label}' å·²è§¸ç™¼ã€‚", icon="ğŸ¤–")

                except Exception as e:
                    st.error(f"è¼‰å…¥ Agent æç¤ºè© '{prompt_file}' å¤±æ•—: {e}")
                    logger.error(f"è¼‰å…¥ Agent æç¤ºè© '{prompt_file}' å¤±æ•—: {e}")
    st.markdown("---") # åˆ†éš”ç·š

    # --- API é‡‘é‘°ç¼ºå¤±è­¦å‘Š (REMOVED as keys are backend managed) ---
    # if not _are_gemini_keys_set(): # This function is also a candidate for removal
    #     st.warning(
    #         "âš ï¸ **æ ¸å¿ƒåŠŸèƒ½ä¾è³´å¾Œç«¯ API é‡‘é‘°é…ç½®**\n\n"
    #         "æ­¤æ‡‰ç”¨ç¨‹å¼çš„æ ¸å¿ƒåˆ†æåŠŸèƒ½ç”± Google Gemini æä¾›æŠ€è¡“æ”¯æ´ã€‚è«‹ç¢ºä¿å¾Œç«¯æœå‹™å·²æ­£ç¢ºé…ç½® API é‡‘é‘°ä»¥å•Ÿç”¨å®Œæ•´åŠŸèƒ½ã€‚\n\n"
    #         "API é‡‘é‘°ä¸å†é€šéå‰ç«¯ç•Œé¢ç®¡ç†ã€‚",
    #         icon="ğŸ”‘"
    #     )
    #     logger.info("ä¸»é é¢ï¼šé¡¯ç¤º API é‡‘é‘°ç”±å¾Œç«¯ç®¡ç†çš„ä¿¡æ¯ã€‚")

    # --- æ­¥é©Ÿä¸€ï¼šä¸Šå‚³æ–‡ä»¶ ---
    st.header("ğŸ“„ æ­¥é©Ÿä¸€ï¼šä¸Šå‚³èˆ‡æª¢è¦–æ–‡ä»¶")
    st.caption("è«‹ä¸Šå‚³æ‚¨éœ€è¦åˆ†æçš„æ–‡å­—æª”æ¡ˆã€CSV æˆ– Excel æª”æ¡ˆåˆ°å¾Œç«¯æœå‹™ã€‚") # Caption updated
    logger.debug("ä¸»é é¢ï¼šæ¸²æŸ“æ–‡ä»¶ä¸Šå‚³å€åŸŸã€‚")
    ui_settings = st.session_state.get("ui_settings", {}) # Ensure ui_settings is available
    allowed_upload_types_fallback = ["txt", "csv", "md", "py", "json", "xml", "html", "css", "js", "xls", "xlsx"] # More comprehensive static fallback
    allowed_upload_file_types_from_settings = ui_settings.get("allowed_upload_file_types", allowed_upload_types_fallback)

    uploaded_files_from_widget = st.file_uploader(
        "ä¸Šå‚³æ–°æ–‡ä»¶åˆ°å¾Œç«¯æœå‹™ (å¯å¤šé¸):", # Label updated
        type=allowed_upload_file_types_from_settings,
        accept_multiple_files=True,
        key="main_file_uploader_widget",
        help="æ­¤è™•ä¸Šå‚³çš„æ–‡ä»¶å°‡ç™¼é€åˆ°å¾Œç«¯é€²è¡Œè™•ç†å’Œå­˜å„²ã€‚" # Help updated
    )

    if uploaded_files_from_widget: # User has selected files in the widget
        logger.info(f"ä¸»é é¢ï¼šç”¨æˆ¶åœ¨ file_uploader ä¸­é¸æ“‡äº† {len(uploaded_files_from_widget)} å€‹æª”æ¡ˆã€‚")

        # Prepare files for multipart/form-data upload
        files_for_api_upload = []
        for up_file in uploaded_files_from_widget:
            files_for_api_upload.append(('files', (up_file.name, up_file.getvalue(), up_file.type)))

        if files_for_api_upload:
            backend_upload_url = f"{app_settings.FASTAPI_BACKEND_URL}/api/files/upload"
            st.info(f"æ­£åœ¨ä¸Šå‚³ {len(files_for_api_upload)} å€‹æ–‡ä»¶åˆ°å¾Œç«¯æœå‹™...")
            try:
                api_response = requests.post(backend_upload_url, files=files_for_api_upload, timeout=180) # Increased timeout for potentially large files
                api_response.raise_for_status() # Raise HTTPError for bad responses (4xx or 5xx)
                response_json = api_response.json()

                if response_json.get("success"):
                    st.success(response_json.get("message", f"æˆåŠŸä¸Šå‚³ {len(files_for_api_upload)} å€‹æ–‡ä»¶åˆ°å¾Œç«¯ã€‚"))

                    # Replace old session state vars for uploaded file contents
                    st.session_state.pop("uploaded_files_list", None)
                    st.session_state.pop("uploaded_file_contents", None)

                    # Store new backend-provided file info
                    # It's better to append to or update an existing list to handle multiple upload batches.
                    if "uploaded_documents_info" not in st.session_state:
                        st.session_state.uploaded_documents_info = []

                    new_files_info = response_json.get("uploaded_files_info", [])
                    # Simple append for now. A more robust way would be to update by file_id if files can be re-uploaded.
                    st.session_state.uploaded_documents_info.extend(new_files_info)

                    logger.info(f"å¾Œç«¯æ–‡ä»¶ä¸Šå‚³æˆåŠŸã€‚æ”¶åˆ° {len(new_files_info)} å€‹æ–‡ä»¶çš„ä¿¡æ¯ã€‚")
                    # Rerun to clear the file_uploader and reflect new state
                    st.rerun()
                else:
                    error_msg = response_json.get("error", "å¾Œç«¯è™•ç†æ–‡ä»¶ä¸Šå‚³å¤±æ•—ï¼Œä½†æœªæä¾›æ˜ç¢ºéŒ¯èª¤è¨Šæ¯ã€‚")
                    st.error(f"å¾Œç«¯éŒ¯èª¤: {error_msg}")
                    logger.error(f"å¾Œç«¯æ–‡ä»¶ä¸Šå‚³APIè¿”å›å¤±æ•—: {error_msg}")

            except requests.exceptions.RequestException as e:
                st.error(f"ä¸Šå‚³æ–‡ä»¶åˆ°å¾Œç«¯å¤±æ•—: {e}")
                logger.error(f"ä¸Šå‚³æ–‡ä»¶åˆ°å¾Œç«¯å¤±æ•—: {e}", exc_info=True)
            except Exception as e:
                st.error(f"è™•ç†æ–‡ä»¶ä¸Šå‚³æ™‚ç™¼ç”ŸæœªçŸ¥éŒ¯èª¤: {e}")
                logger.error(f"è™•ç†æ–‡ä»¶ä¸Šå‚³æ™‚ç™¼ç”ŸæœªçŸ¥éŒ¯èª¤: {e}", exc_info=True)
        # It's important to clear the file uploader widget after an upload attempt (success or failure)
        # to prevent re-uploading the same files on next script run if not handled carefully.
        # A common way is st.rerun(), or manually resetting the key of file_uploader if possible,
        # or managing a flag. st.rerun() is used on success. If error, files remain in widget.

    # Display files based on new session state variable `st.session_state.uploaded_documents_info`
    if st.session_state.get("uploaded_documents_info"):
        st.subheader("å·²ä¸Šå‚³åˆ°å¾Œç«¯çš„æ–‡ä»¶:")
        # Create a set of filenames for efficient duplicate checking if backend doesn't ensure uniqueness
        # For now, assuming backend returns a clean list per upload batch, and we extend.
        # If backend ensures global uniqueness by file_id, list directly.

        displayed_files = st.session_state.uploaded_documents_info
        if not displayed_files:
             st.caption("ç›®å‰æ²’æœ‰æ–‡ä»¶ä¿¡æ¯ã€‚")

        for doc_info in displayed_files:
            file_id = doc_info.get("file_id", "N/A")
            filename = doc_info.get("filename", "N/A")
            summary = doc_info.get("summary", "å¾Œç«¯æœªæä¾›é è¦½ã€‚")
            content_type = doc_info.get("content_type", "N/A")
            size_kb = doc_info.get("size", 0) / 1024

            expander_title = f"æ–‡ä»¶å: {filename} (ID: ...{file_id[-6:] if file_id != 'N/A' else 'N/A'}, ç±»å‹: {content_type}, å¤§å°: {size_kb:.2f} KB)"
            with st.expander(expander_title, expanded=False):
                st.markdown(f"**å¾Œç«¯æä¾›çš„æ‘˜è¦/é è¦½:**")
                st.text(summary)
                # TODO: Add a button or mechanism to remove/manage individual files if backend supports it
                # e.g., if st.button(f"å¾å¾Œç«¯ç§»é™¤ {filename}", key=f"del_{file_id}"): call_backend_delete_api(file_id)
    else:
        st.info("å°šæœªä¸Šå‚³ä»»ä½•æ–‡ä»¶åˆ°å¾Œç«¯æœå‹™ã€‚è«‹ä½¿ç”¨ä¸Šæ–¹çš„ä¸Šå‚³å·¥å…·ã€‚") # Updated message
    st.markdown("---")

    # --- æ­¥é©ŸäºŒï¼šé¸æ“‡æ ¸å¿ƒåˆ†ææ–‡ä»¶ (ç”¨æ–¼ All-in-One å ±å‘Š) ---
    st.header("ğŸ¯ æ­¥é©ŸäºŒï¼šé¸æ“‡æ ¸å¿ƒåˆ†ææ–‡ä»¶ (ç”¨æ–¼ All-in-One å ±å‘Š)")
    st.caption("å¾ `Wolf_Data/source_documents/` (æœ¬åœ°æƒæ) æˆ–å·²ä¸Šå‚³åˆ°å¾Œç«¯çš„æ–‡ä»¶ä¸­é¸æ“‡æ ¸å¿ƒæ–‡æª”ã€‚") # Caption updated
    logger.debug("ä¸»é é¢ï¼šæ¸²æŸ“æ ¸å¿ƒåˆ†ææ–‡ä»¶é¸æ“‡å€åŸŸã€‚")

    # --- Wolf_Data/source_documents/ æ–‡ä»¶æƒæé‚è¼¯ (Frontend local scan) ---
    # This part remains largely the same if SOURCE_DOCS_DIR is a locally accessible path by the frontend.
    # The `ensure_colab_drive_mount_if_needed` is still relevant for this local scanning part.
    can_scan_wolf_data = True
    if IN_COLAB:
        if not ensure_colab_drive_mount_if_needed(SOURCE_DOCS_DIR):
            can_scan_wolf_data = False
            logger.info(f"ä¸»é é¢ï¼šensure_colab_drive_mount_if_needed è¿”å› False for '{SOURCE_DOCS_DIR}'. è·³éæƒæã€‚")
            st.session_state.wolf_data_file_map = {} # Ensure it's empty if scan is skipped

    local_wolf_data_files_map = {} # Stores display_name -> {type, identifier, display_name}
    core_doc_scan_extensions_fallback = (".txt", ".md")
    core_doc_scan_extensions_from_settings = ui_settings.get("core_doc_scan_extensions", core_doc_scan_extensions_fallback)

    if can_scan_wolf_data and os.path.exists(SOURCE_DOCS_DIR):
        logger.info(f"ä¸»é é¢ï¼šé–‹å§‹æƒææœ¬åœ° Wolf_Data ç›®éŒ„: {SOURCE_DOCS_DIR}")
        for root, _, files in os.walk(SOURCE_DOCS_DIR):
            for file in files:
                if file.endswith(core_doc_scan_extensions_from_settings):
                    full_path = os.path.join(root, file)
                    relative_path = os.path.relpath(full_path, SOURCE_DOCS_DIR)
                    display_name = f"[æœ¬åœ°] {relative_path}"
                    local_wolf_data_files_map[display_name] = {"path_type": "local_wolf_data", "identifier": full_path, "display_name": display_name}
        logger.info(f"ä¸»é é¢ï¼šæƒæåˆ° {len(local_wolf_data_files_map)} å€‹æ–‡ä»¶å¾æœ¬åœ° Wolf_Dataã€‚")
    elif can_scan_wolf_data and not os.path.exists(SOURCE_DOCS_DIR): # Path doesn't exist, but tried to scan (e.g. local non-Colab)
        logger.warning(f"ä¸»é é¢ï¼šæœ¬åœ° Wolf_Data ç›®éŒ„ '{SOURCE_DOCS_DIR}' ä¸å­˜åœ¨ã€‚")
        # st.session_state.wolf_data_file_map might not be the right variable to clear here if it's specific to previous structure

    # --- Files uploaded to backend ---
    backend_uploaded_files_map = {} # Stores display_name -> {type, identifier, display_name}
    if st.session_state.get("uploaded_documents_info"):
        for doc_info in st.session_state.uploaded_documents_info:
            filename = doc_info.get('filename', 'N/A')
            file_id = doc_info.get('file_id', 'N/A')
            display_name = f"[å¾Œç«¯] {filename} (ID: ...{file_id[-6:] if file_id != 'N/A' else 'N/A'})"
            backend_uploaded_files_map[display_name] = {"path_type": "backend_file_id", "identifier": file_id, "display_name": display_name, "filename": filename}

    # Combine options for multiselect
    # The keys of this map will be the display names shown in the multiselect widget
    combined_options_map = {**local_wolf_data_files_map, **backend_uploaded_files_map}
    multiselect_display_options = sorted(list(combined_options_map.keys()))

    if not multiselect_display_options:
        st.info(f"æç¤ºï¼šåœ¨ '{SOURCE_DOCS_DIR}' (æœ¬åœ°) ä¸­æœªæ‰¾åˆ°ç¬¦åˆæ¢ä»¶çš„æ–‡ä»¶ï¼Œä¸”å¾Œç«¯ç„¡å·²ä¸Šå‚³æ–‡ä»¶ã€‚è«‹ä½¿ç”¨ã€Œæ­¥é©Ÿä¸€ã€ä¸Šå‚³æ–‡ä»¶ã€‚")
        # Provide a dummy option if empty to prevent multiselect error, though it's better if multiselect handles empty options list gracefully.
        # For now, assuming st.multiselect handles empty list fine, or user sees the info message.
    else:
        st.info(f"è«‹å¾æƒæåˆ°çš„æœ¬åœ° '{os.path.basename(SOURCE_DOCS_DIR.rstrip(os.sep))}' ç›®éŒ„æˆ–å·²ä¸Šå‚³åˆ°å¾Œç«¯çš„æ–‡ä»¶ä¸­é¸æ“‡ã€‚")

    # `selected_core_documents` in session_state will now store the list of selected *display names*.
    # We will then use `combined_options_map` to get the actual identifier and type when needed (e.g., for building context).

    # Get current selections from session state (these are display names)
    # Default selection logic needs to be robust if "selected_core_documents" previously stored different kind of values
    # For this refactor, let's assume selected_core_documents will store the display names.
    # If it's the first run or structure changed, it might be empty or need conversion.

    # Ensure st.session_state.selected_core_documents holds display names compatible with current options
    current_default_selection = []
    if "selected_core_documents" in st.session_state:
        if isinstance(st.session_state.selected_core_documents, list):
            # Filter to ensure only valid display names are part of default
            current_default_selection = [name for name in st.session_state.selected_core_documents if name in multiselect_display_options]
        else: # If it was not a list, reset it
            st.session_state.selected_core_documents = []


    selected_display_names_from_widget = st.multiselect(
        "é¸æ“‡è¦é€²è¡Œ All-in-One ç¶œåˆåˆ†æçš„æ–‡ä»¶ (å¯è¤‡é¸):",
        options=multiselect_display_options, # List of display names
        default=current_default_selection,
        key="main_selected_core_documents_multiselect_new" # New key to avoid state conflicts if old one existed with different data type
    )

    if selected_display_names_from_widget != st.session_state.get("selected_core_documents"):
        st.session_state.selected_core_documents = selected_display_names_from_widget # Store the selected display names
        logger.info(f"ä¸»é é¢ï¼šç”¨æˆ¶é¸æ“‡çš„æ ¸å¿ƒåˆ†ææ–‡ä»¶ (é¡¯ç¤ºåç¨±) å·²æ›´æ–°: {selected_display_names_from_widget}")

    if st.session_state.get("selected_core_documents"):
        st.markdown("##### å·²é¸å®šæ ¸å¿ƒæ–‡ä»¶:")
        for display_name in st.session_state.selected_core_documents:
            # Retrieve the full info for logging or more detailed display if needed
            doc_detail = combined_options_map.get(display_name)
            if doc_detail:
                 st.markdown(f"- {doc_detail['display_name']} (ä¾†æº: {doc_detail['path_type']})")
            else: # Should not happen if default filtering is correct
                 st.markdown(f"- {display_name} (è©³ç´°è³‡è¨ŠæœªçŸ¥)")
    else:
        st.caption("å°šæœªé¸å®šä»»ä½•æ ¸å¿ƒæ–‡ä»¶ã€‚")

    st.markdown("---")


    # --- æ­¥é©Ÿä¸‰ï¼šå¼•å…¥å¤–éƒ¨æ•¸æ“š ---
    st.header("ğŸ“Š æ­¥é©Ÿä¸‰ï¼šå¼•å…¥å¤–éƒ¨æ•¸æ“š (å¯é¸)")
    logger.debug("ä¸»é é¢ï¼šæ¸²æŸ“å¤–éƒ¨æ•¸æ“šæºé¸æ“‡å€åŸŸã€‚")
    with st.expander("ğŸ“Š è¨­å®šä¸¦è¼‰å…¥å¤–éƒ¨æ•¸æ“š (é»æ“Šå±•é–‹/æ”¶èµ·)", expanded=False):
        st.caption("é¸æ“‡ yfinance, FRED, NY Fed ç­‰å¤–éƒ¨å¸‚å ´èˆ‡ç¸½ç¶“æ•¸æ“šæºï¼Œè¨­å®šåƒæ•¸ä¸¦è¼‰å…¥æ•¸æ“šã€‚")

        # ç¢ºä¿ç›¸é—œ session_state éµå·²ç”± session_state_manager åˆå§‹åŒ–
        # æ­¤è™•ä¸å†é‡è¤‡åˆå§‹åŒ–ï¼Œä»¥ä¾è³´ manager çš„çµ±ä¸€ç®¡ç†

        col_sel1, col_sel2, col_sel3, col_sel4 = st.columns(4)
        with col_sel1:
            old_select_all = st.session_state.get("select_all_sources", False)
            new_select_all = st.checkbox("ğŸŒ å…¨é¸æ•¸æ“šæº", value=old_select_all, key="main_select_all_cb")
            if new_select_all != old_select_all:
                st.session_state.select_all_sources = new_select_all
                st.session_state.select_yfinance = new_select_all
                st.session_state.select_fred = new_select_all
                st.session_state.select_ny_fed = new_select_all
                logger.info(f"ä¸»é é¢ï¼šç”¨æˆ¶æ›´æ”¹'å…¨é¸æ•¸æ“šæº'ç‚º {new_select_all}ã€‚")
                st.rerun()
        with col_sel2:
            old_yf = st.session_state.get("select_yfinance", False)
            new_yf = st.checkbox("ğŸ“ˆ yfinance", value=old_yf, key="main_select_yfinance_cb")
            if new_yf != old_yf:
                st.session_state.select_yfinance = new_yf
                logger.info(f"ä¸»é é¢ï¼šç”¨æˆ¶æ›´æ”¹'yfinance'é¸æ“‡ç‚º {new_yf}ã€‚")
        with col_sel3:
            old_fred = st.session_state.get("select_fred", False)
            new_fred = st.checkbox("ğŸ¦ FRED", value=old_fred, key="main_select_fred_cb")
            if new_fred != old_fred:
                st.session_state.select_fred = new_fred
                logger.info(f"ä¸»é é¢ï¼šç”¨æˆ¶æ›´æ”¹'FRED'é¸æ“‡ç‚º {new_fred}ã€‚")
        with col_sel4:
            old_nyf = st.session_state.get("select_ny_fed", False)
            new_nyf = st.checkbox("ğŸ‡ºğŸ‡¸ NY Fed", value=old_nyf, key="main_select_nyfed_cb")
            if new_nyf != old_nyf:
                st.session_state.select_ny_fed = new_nyf
                logger.info(f"ä¸»é é¢ï¼šç”¨æˆ¶æ›´æ”¹'NY Fed'é¸æ“‡ç‚º {new_nyf}ã€‚")


        date_col1, date_col2 = st.columns(2)
        with date_col1:
            old_start_date = st.session_state.get("data_start_date", "")
            new_start_date = st.text_input("é–‹å§‹æ—¥æœŸ (YYYYMMDD):", value=old_start_date, key="main_data_start_date")
            if new_start_date != old_start_date:
                st.session_state.data_start_date = new_start_date
                logger.debug(f"ä¸»é é¢ï¼šæ•¸æ“šé–‹å§‹æ—¥æœŸæ›´æ”¹ç‚º {new_start_date}")
        with date_col2:
            old_end_date = st.session_state.get("data_end_date", "")
            new_end_date = st.text_input("çµæŸæ—¥æœŸ (YYYYMMDD):", value=old_end_date, key="main_data_end_date")
            if new_end_date != old_end_date:
                st.session_state.data_end_date = new_end_date
                logger.debug(f"ä¸»é é¢ï¼šæ•¸æ“šçµæŸæ—¥æœŸæ›´æ”¹ç‚º {new_end_date}")

        param_col1, param_col2 = st.columns(2)
        with param_col1:
            if st.session_state.get("select_yfinance"):
                default_yf_tickers = ui_settings.get("default_yfinance_tickers", "SPY,QQQ")
                old_yf_tickers = st.session_state.get("yfinance_tickers", default_yf_tickers)
                new_yf_tickers = st.text_input("yfinance Tickers (é€—è™Ÿåˆ†éš”):", value=old_yf_tickers, key="main_yfinance_tickers")
                if new_yf_tickers != old_yf_tickers:
                    st.session_state.yfinance_tickers = new_yf_tickers
                    logger.debug(f"ä¸»é é¢ï¼šyfinance tickers æ›´æ”¹ç‚º '{new_yf_tickers}'")

                yfinance_interval_options_fallback = {"1 Day": "1d", "1 Week": "1wk"}
                yfinance_interval_options_from_settings = ui_settings.get("yfinance_interval_options", yfinance_interval_options_fallback)
                default_yfinance_interval_label_from_settings = ui_settings.get("default_yfinance_interval_label", "1 Day")

                old_yf_interval_label = st.session_state.get("yfinance_interval_label", default_yfinance_interval_label_from_settings)
                new_yf_interval_label = st.selectbox(
                    "yfinance æ•¸æ“šé€±æœŸ:",
                    options=list(yfinance_interval_options_from_settings.keys()),
                    index=list(yfinance_interval_options_from_settings.keys()).index(old_yf_interval_label) if old_yf_interval_label in yfinance_interval_options_from_settings else 0,
                    key="main_yfinance_interval"
                )
                if new_yf_interval_label != old_yf_interval_label:
                    st.session_state.yfinance_interval_label = new_yf_interval_label
                    logger.debug(f"ä¸»é é¢ï¼šyfinance interval æ›´æ”¹ç‚º '{new_yf_interval_label}'")
        with param_col2:
            if st.session_state.get("select_fred"):
                default_fred_ids = ui_settings.get("default_fred_series_ids", "GDP,CPIAUCSL")
                old_fred_ids = st.session_state.get("fred_series_ids", default_fred_ids)
                new_fred_ids = st.text_input("FRED Series IDs (é€—è™Ÿåˆ†éš”):", value=old_fred_ids, key="main_fred_ids")
                if new_fred_ids != old_fred_ids:
                    st.session_state.fred_series_ids = new_fred_ids
                    logger.debug(f"ä¸»é é¢ï¼šFRED Series IDs æ›´æ”¹ç‚º '{new_fred_ids}'")

        if st.button("ğŸ” ç²å–ä¸¦é è¦½é¸å®šæ•¸æ“š", key="main_fetch_data_button"):
            logger.info("ä¸»é é¢ï¼šç”¨æˆ¶é»æ“Š 'ç²å–ä¸¦é è¦½é¸å®šæ•¸æ“š' æŒ‰éˆ•ã€‚")
            st.session_state.fetch_data_button_clicked = True # Mark as clicked to persist state across reruns if needed
            st.session_state.fetched_data_preview = {} # Clear previous previews
            st.session_state.fetch_errors = {} # Clear previous errors

            if not (st.session_state.get("select_yfinance") or st.session_state.get("select_fred") or st.session_state.get("select_ny_fed")):
                logger.warning("ä¸»é é¢ï¼šç”¨æˆ¶é»æ“Šç²å–æ•¸æ“šä½†æœªé¸æ“‡ä»»ä½•æ•¸æ“šæºã€‚")
                st.warning("è«‹è‡³å°‘é¸æ“‡ä¸€å€‹æ•¸æ“šæºã€‚")
                st.session_state.fetch_data_button_clicked = False # Reset if no action taken
            else:
                with st.spinner("æ­£åœ¨ç²å–å¤–éƒ¨æ•¸æ“š...è«‹ç¨å€™..."):
                    st.toast("â³ æ­£åœ¨å¾å¤–éƒ¨æºç²å–æ•¸æ“šï¼Œè«‹è€å¿ƒç­‰å€™...", icon="â³")
                    logger.debug("ä¸»é é¢ï¼šé–‹å§‹ç²å–æ‰€é¸å¤–éƒ¨æ•¸æ“šã€‚")
                    if st.session_state.get("select_yfinance"):
                        yfinance_interval_options_from_settings = ui_settings.get("yfinance_interval_options", {"1 Day": "1d"}) # Fallback
                        actual_yf_interval = yfinance_interval_options_from_settings.get(st.session_state.yfinance_interval_label, "1d") # Fallback interval value
                        current_yf_tickers = st.session_state.get("yfinance_tickers", ui_settings.get("default_yfinance_tickers", "SPY"))
                        logger.info(f"ä¸»é é¢ï¼šæº–å‚™èª¿ç”¨å¾Œç«¯ YFinance APIã€‚Tickers: {current_yf_tickers}, Start: {st.session_state.data_start_date}, End: {st.session_state.data_end_date}, Interval: {actual_yf_interval}")

                        yfinance_payload = {
                            "tickers": [t.strip() for t in current_yf_tickers.split(",")],
                            "start_date": st.session_state.data_start_date,
                            "end_date": st.session_state.data_end_date,
                            "interval": actual_yf_interval
                        }
                        try:
                            response = requests.post(f"{app_settings.FASTAPI_BACKEND_URL}/api/data/yfinance", json=yfinance_payload, timeout=60)
                            response.raise_for_status()
                            yf_response_json = response.json()
                            if yf_response_json.get("success"):
                                yf_data_json = yf_response_json.get("data", {})
                                yf_data_dfs = {ticker: pd.read_json(pds_json, orient='split') for ticker, pds_json in yf_data_json.items()}
                                if yf_data_dfs: st.session_state.fetched_data_preview["yfinance"] = yf_data_dfs
                                logger.info(f"å¾Œç«¯ YFinance API èª¿ç”¨æˆåŠŸã€‚ç²å– {len(yf_data_dfs)} çµ„æ•¸æ“šã€‚")
                                # Handle plots if backend provides them
                                # if "plots" in yf_response_json: ...
                            else:
                                err_msg = yf_response_json.get("error", "YFinance å¾Œç«¯ API è¿”å›å¤±æ•—ä½†æœªæä¾›éŒ¯èª¤è¨Šæ¯ã€‚")
                                st.session_state.fetch_errors["yfinance"] = [err_msg]
                                logger.error(f"å¾Œç«¯ YFinance API è¿”å›å¤±æ•—: {err_msg}")
                        except requests.exceptions.RequestException as e:
                            st.session_state.fetch_errors["yfinance"] = [f"èª¿ç”¨ YFinance å¾Œç«¯ API å¤±æ•—: {e}"]
                            logger.error(f"èª¿ç”¨ YFinance å¾Œç«¯ API å¤±æ•—: {e}", exc_info=True)

                    if st.session_state.get("select_fred"):
                        current_fred_ids = st.session_state.get("fred_series_ids", ui_settings.get("default_fred_series_ids", "GDP"))
                        logger.info(f"ä¸»é é¢ï¼šæº–å‚™èª¿ç”¨å¾Œç«¯ FRED APIã€‚Series IDs: {current_fred_ids}, Start: {st.session_state.data_start_date}, End: {st.session_state.data_end_date}")
                        fred_payload = {
                            "series_ids": [s.strip() for s in current_fred_ids.split(",")],
                            "start_date": st.session_state.data_start_date,
                            "end_date": st.session_state.data_end_date
                        }
                        try:
                            response = requests.post(f"{app_settings.FASTAPI_BACKEND_URL}/api/data/fred", json=fred_payload, timeout=60)
                            response.raise_for_status()
                            fred_response_json = response.json()
                            if fred_response_json.get("success"):
                                fred_data_json = fred_response_json.get("data", {})
                                fred_data_dfs = {sid: pd.read_json(s_json, orient='split') for sid, s_json in fred_data_json.items()}
                                if fred_data_dfs: st.session_state.fetched_data_preview["fred"] = fred_data_dfs
                                logger.info(f"å¾Œç«¯ FRED API èª¿ç”¨æˆåŠŸã€‚ç²å– {len(fred_data_dfs)} çµ„æ•¸æ“šã€‚")
                            else:
                                err_msg = fred_response_json.get("error", "FRED å¾Œç«¯ API è¿”å›å¤±æ•—ä½†æœªæä¾›éŒ¯èª¤è¨Šæ¯ã€‚")
                                st.session_state.fetch_errors["fred"] = [err_msg]
                                logger.error(f"å¾Œç«¯ FRED API è¿”å›å¤±æ•—: {err_msg}")
                        except requests.exceptions.RequestException as e:
                            st.session_state.fetch_errors["fred"] = [f"èª¿ç”¨ FRED å¾Œç«¯ API å¤±æ•—: {e}"]
                            logger.error(f"èª¿ç”¨ FRED å¾Œç«¯ API å¤±æ•—: {e}", exc_info=True)

                    if st.session_state.get("select_ny_fed"):
                        logger.info("ä¸»é é¢ï¼šæº–å‚™èª¿ç”¨å¾Œç«¯ NY Fed (fetch_web_content) APIã€‚")
                        # This URL might need to be made configurable or passed differently if it changes often
                        ny_fed_target_url = "https://www.newyorkfed.org/markets/desk-operations/ambs" # Example, assuming this is what was fetched
                        nyfed_payload = {"url": ny_fed_target_url, "source_type": "ny_fed_ambs"} # Added source_type for backend
                        try:
                            response = requests.post(f"{app_settings.FASTAPI_BACKEND_URL}/api/data/fetch_web_content", json=nyfed_payload, timeout=60)
                            response.raise_for_status()
                            nyfed_response_json = response.json()
                            if nyfed_response_json.get("success"):
                                # Assuming backend returns DataFrame as JSON string if it was processed to DataFrame
                                # Or raw content if that's what the backend provides
                                if nyfed_response_json.get("content_type") == "application/json_dataframe": # Custom type for DF
                                     nyfed_df = pd.read_json(nyfed_response_json.get("data"), orient='split')
                                     st.session_state.fetched_data_preview["ny_fed"] = nyfed_df
                                     logger.info(f"å¾Œç«¯ NY Fed API èª¿ç”¨æˆåŠŸã€‚ç²å– DataFrameã€‚")
                                else: # Handle as plain text or other content
                                     st.session_state.fetched_data_preview["ny_fed_content"] = nyfed_response_json.get("data","")
                                     logger.info(f"å¾Œç«¯ NY Fed API èª¿ç”¨æˆåŠŸã€‚ç²å– Content-Type: {nyfed_response_json.get('content_type')}")
                            else:
                                err_msg = nyfed_response_json.get("error", "NY Fed å¾Œç«¯ API è¿”å›å¤±æ•—ä½†æœªæä¾›éŒ¯èª¤è¨Šæ¯ã€‚")
                                st.session_state.fetch_errors["ny_fed"] = [err_msg]
                                logger.error(f"å¾Œç«¯ NY Fed API è¿”å›å¤±æ•—: {err_msg}")
                        except requests.exceptions.RequestException as e:
                            st.session_state.fetch_errors["ny_fed"] = [f"èª¿ç”¨ NY Fed å¾Œç«¯ API å¤±æ•—: {e}"]
                            logger.error(f"èª¿ç”¨ NY Fed å¾Œç«¯ API å¤±æ•—: {e}", exc_info=True)

                if not st.session_state.get("fetch_errors") and st.session_state.get("fetched_data_preview"):
                    logger.info("ä¸»é é¢ï¼šå¤–éƒ¨æ•¸æ“šå·²æˆåŠŸç²å– (é€šéå¾Œç«¯ API) ä¸”ç„¡éŒ¯èª¤å ±å‘Šã€‚")
                    st.success("å¤–éƒ¨æ•¸æ“šå·²æˆåŠŸç²å– (é€šéå¾Œç«¯)ã€‚")
                elif not st.session_state.get("fetched_data_preview") and not st.session_state.get("fetch_errors") and not st.session_state.get("fetched_data_preview",{}).get("ny_fed_content"):
                     logger.info("ä¸»é é¢ï¼šæœªç²å–åˆ°ä»»ä½•å¤–éƒ¨æ•¸æ“š (é€šéå¾Œç«¯ API)ï¼Œä¸”ç„¡éŒ¯èª¤å ±å‘Šã€‚")
                     st.info("æœªç²å–åˆ°ä»»ä½•å¤–éƒ¨æ•¸æ“šã€‚è«‹æª¢æŸ¥æ‚¨çš„è¼¸å…¥ã€æ•¸æ“šæºï¼Œæˆ–å¾Œç«¯æœå‹™æ—¥èªŒã€‚")
                # å¦‚æœæœ‰éŒ¯èª¤ï¼Œæœƒåœ¨ä¸‹é¢é¡¯ç¤º

        if st.session_state.get("fetch_errors"):
            logger.warning(f"ä¸»é é¢ï¼šæ•¸æ“šç²å–éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {st.session_state.fetch_errors}")
            st.subheader("æ•¸æ“šç²å–éŒ¯èª¤:")
            for source, errors_list in st.session_state.fetch_errors.items():
                st.error(f"ä¾†æº {source.upper()}:")
                for err_msg in errors_list:
                    st.markdown(f"- {err_msg}")

        if st.session_state.get("fetched_data_preview"):
            logger.debug("ä¸»é é¢ï¼šé–‹å§‹æ¸²æŸ“å·²ç²å–æ•¸æ“šçš„é è¦½ã€‚")
            st.subheader("å·²ç²å–æ•¸æ“šé è¦½:")
            if "yfinance" in st.session_state.fetched_data_preview:
                st.markdown("#### yfinance æ•¸æ“š:")
                for ticker, df_yf in st.session_state.fetched_data_preview["yfinance"].items():
                    with st.expander(f"Ticker: {ticker} (å…± {len(df_yf)} è¡Œ)", expanded=False): st.dataframe(df_yf.head())

            if "fred" in st.session_state.fetched_data_preview:
                st.markdown("#### FRED æ•¸æ“š:")
                for series_id, df_fred in st.session_state.fetched_data_preview["fred"].items():
                    with st.expander(f"Series ID: {series_id} (å…± {len(df_fred)} è¡Œ)", expanded=False): st.dataframe(df_fred.head())

            if "ny_fed" in st.session_state.fetched_data_preview and isinstance(st.session_state.fetched_data_preview["ny_fed"], pd.DataFrame) and not st.session_state.fetched_data_preview["ny_fed"].empty:
                st.markdown("#### NY Fed ä¸€ç´šäº¤æ˜“å•†æ•¸æ“š (DataFrame):")
                df_nyfed = st.session_state.fetched_data_preview["ny_fed"]
                with st.expander(f"NY Fed ç¸½æŒæœ‰é‡ (å…± {len(df_nyfed)} è¡Œ)", expanded=False): st.dataframe(df_nyfed.head())
            elif "ny_fed_content" in st.session_state.fetched_data_preview and st.session_state.fetched_data_preview["ny_fed_content"]:
                st.markdown("#### NY Fed æ•¸æ“š (åŸå§‹å…§å®¹):")
                with st.expander("åŸå§‹ç¶²é å…§å®¹é è¦½ (å‰500å­—ç¬¦)", expanded=False):
                    st.text(st.session_state.fetched_data_preview["ny_fed_content"][:500] + "...")
            logger.debug("ä¸»é é¢ï¼šå·²ç²å–æ•¸æ“šé è¦½æ¸²æŸ“å®Œç•¢ (é€šéå¾Œç«¯ API)ã€‚")

    st.markdown("---")
    logger.info("ä¸»é é¢ï¼šä¸»è¦å…§å®¹æ¸²æŸ“çµæŸ (render_main_page_content)ã€‚")

if __name__ == "__main__":
    # æ¨¡æ“¬æ¸¬è©¦ (éœ€è¦ Streamlit ç’°å¢ƒå’Œ session_state)
    # st.session_state.update({ ... åˆå§‹åŒ– main_page éœ€è¦çš„éµ ...})
    # render_main_page_content()
    pass
